{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a54c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('Basics_pyspark_df').getOrCreate()\n",
    "df = spark.read.csv('data/student_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8149bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-------+-------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|\n",
      "+-------+-------+-----+-----+-------+-------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|\n",
      "|   John|      3|   23|    9|  Maths|      B|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|\n",
      "|Nicolas|      5|   67|    9|English|      A|\n",
      "+-------+-------+-----+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3929a7",
   "metadata": {},
   "source": [
    "### Select vs Collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f191200",
   "metadata": {},
   "source": [
    "select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f0596",
   "metadata": {},
   "source": [
    "df.collect() retrieves all elements in a DataFrame as an Array of Row type to the driver node. Note that collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to process it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ded8cc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Avind', Roll_No=1, Marks=92, Class=9, Subject='Maths', Section='A'),\n",
       " Row(Name='Aditya', Roll_No=2, Marks=87, Class=9, Subject='Maths', Section='A'),\n",
       " Row(Name='John', Roll_No=3, Marks=23, Class=9, Subject='Maths', Section='B'),\n",
       " Row(Name='Mary', Roll_No=4, Marks=45, Class=9, Subject='Maths', Section='B'),\n",
       " Row(Name='Nicolas', Roll_No=5, Marks=67, Class=9, Subject='English', Section='A'),\n",
       " Row(Name='Jonny', Roll_No=6, Marks=100, Class=9, Subject='English', Section='A'),\n",
       " Row(Name='Tom', Roll_No=7, Marks=55, Class=9, Subject='English', Section='B'),\n",
       " Row(Name='Yash', Roll_No=4, Marks=32, Class=9, Subject='English', Section='B'),\n",
       " Row(Name='Pushkar', Roll_No=8, Marks=30, Class=9, Subject='Science', Section='B'),\n",
       " Row(Name='Parth', Roll_No=9, Marks=76, Class=9, Subject='Science', Section='A'),\n",
       " Row(Name='Piyush', Roll_No=10, Marks=86, Class=9, Subject='Science', Section='A'),\n",
       " Row(Name='Zodiac', Roll_No=11, Marks=65, Class=9, Subject='Computer', Section='B')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2041655a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "725f3ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avind_A\n",
      "Aditya_A\n",
      "John_B\n",
      "Mary_B\n",
      "Nicolas_A\n",
      "Jonny_A\n",
      "Tom_B\n",
      "Yash_B\n",
      "Pushkar_B\n",
      "Parth_A\n",
      "Piyush_A\n",
      "Zodiac_B\n"
     ]
    }
   ],
   "source": [
    "dataCollect = df.collect()\n",
    "for data in dataCollect:\n",
    "    print(data['Name']+\"_\"+data['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "134dcf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Roll_No: int, Marks: int, Class: int, Subject: string, Section: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1d86ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed97cfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-------+-------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|\n",
      "+-------+-------+-----+-----+-------+-------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|\n",
      "|   John|      3|   23|    9|  Maths|      B|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|\n",
      "|Nicolas|      5|   67|    9|English|      A|\n",
      "+-------+-------+-----+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ecdd6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+------------------------+\n",
      "|name                    |\n",
      "+------------------------+\n",
      "|{John, , Don}           |\n",
      "|{Mahesh, Raj, }         |\n",
      "|{Robert, Dorney, Junior}|\n",
      "|{May, , Jones}          |\n",
      "|{Nick, Mary, Brown}     |\n",
      "+------------------------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|John     |Don     |\n",
      "|Mahesh   |        |\n",
      "|Robert   |Junior  |\n",
      "|May      |Jones   |\n",
      "|Nick     |Brown   |\n",
      "+---------+--------+\n",
      "\n",
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "|John     |          |Don     |\n",
      "|Mahesh   |Raj       |        |\n",
      "|Robert   |Dorney    |Junior  |\n",
      "|May      |          |Jones   |\n",
      "|Nick     |Mary      |Brown   |\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select for nested column\n",
    "\n",
    "structureData = [((\"John\",\"\",\"Don\"),36636.334,\"M\",3000),\n",
    "    ((\"Mahesh\",\"Raj\",\"\"),40288.101,\"M\",4000),\n",
    "    ((\"Robert\",\"Dorney\",\"Junior\"),42114.99,\"M\",4000),\n",
    "    ((\"May\",\"\",\"Jones\"),3919.00123,\"F\",4000),\n",
    "    ((\"Nick\",\"Mary\",\"Brown\"),np.nan,\"F\",-1000)]\n",
    "\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.printSchema()\n",
    "\n",
    "df2.select(\"name\").show(truncate=False)\n",
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)\n",
    "df2.select(\"name.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21033945",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2167f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "172fe243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04b27c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select(['Name','Roll_No']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05ce00b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   Name|Roll_No|\n",
      "+-------+-------+\n",
      "|  Avind|      1|\n",
      "| Aditya|      2|\n",
      "|   John|      3|\n",
      "|   Mary|      4|\n",
      "|Nicolas|      5|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Name','Roll_No']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319ba72",
   "metadata": {},
   "source": [
    "### Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04d72b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Avind', Roll_No=1, Marks=92, Class=9, Subject='Maths', Section='A'),\n",
       " Row(Name='Aditya', Roll_No=2, Marks=87, Class=9, Subject='Maths', Section='A')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18661705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "931fca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Avind', Roll_No=1, Marks=92, Class=9, Subject='Maths', Section='A')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]    #fetch row level data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed0926",
   "metadata": {},
   "source": [
    "### withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c21094",
   "metadata": {},
   "source": [
    "PySpark withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27af1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change dtype\n",
    "\n",
    "df = df.withColumn(\"Class_changed_dtype\",col(\"Class\").cast(\"String\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df1e2acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Roll_No: integer (nullable = true)\n",
      " |-- Marks: integer (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- Class_changed_dtype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db225f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-------+-------+-------------------+------------------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|Class_changed_dtype|     Marks_Percent|\n",
      "+-------+-------+-----+-----+-------+-------+-------------------+------------------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|                  9| 61.33333333333333|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|                  9| 57.99999999999999|\n",
      "|   John|      3|   23|    9|  Maths|      B|                  9|15.333333333333332|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|                  9|              30.0|\n",
      "|Nicolas|      5|   67|    9|English|      A|                  9|44.666666666666664|\n",
      "+-------+-------+-----+-----+-------+-------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change value of a column\n",
    "\n",
    "df.withColumn(\"Marks_Percent\",col(\"Marks\")/150*100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ade61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-------+-------+-------+--------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|Country|New_col2|\n",
      "+-------+-------+-----+-----+-------+-------+-------+--------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|  India|      10|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|  India|      10|\n",
      "|   John|      3|   23|    9|  Maths|      B|  India|      10|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|  India|      10|\n",
      "|Nicolas|      5|   67|    9|English|      A|  India|      10|\n",
      "+-------+-------+-----+-----+-------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark lit() function is used to add a constant value to a DataFrame column.\n",
    "#We can also chain in order to add multiple columns.\n",
    "\n",
    "df.withColumn('Country',lit('India')).withColumn('New_col2', col('Class')+1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58366ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-------+-------+---------------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|Marks_mul_class|\n",
      "+-------+-------+-----+-----+-------+-------+---------------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|            828|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|            783|\n",
      "|   John|      3|   23|    9|  Maths|      B|            207|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|            405|\n",
      "|Nicolas|      5|   67|    9|English|      A|            603|\n",
      "+-------+-------+-----+-----+-------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------+-----+-----+-------+-------+------------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|Name_section|\n",
      "+-------+-------+-----+-----+-------+-------+------------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|      AvindA|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|     AdityaA|\n",
      "|   John|      3|   23|    9|  Maths|      B|       JohnB|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|       MaryB|\n",
      "|Nicolas|      5|   67|    9|English|      A|    NicolasA|\n",
      "+-------+-------+-----+-----+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------+-----+-----+-------+-------+------------+\n",
      "|   Name|Roll_No|Marks|Class|Subject|Section|Name_section|\n",
      "+-------+-------+-----+-----+-------+-------+------------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|     Avind_A|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|    Aditya_A|\n",
      "|   John|      3|   23|    9|  Maths|      B|      John_B|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|      Mary_B|\n",
      "|Nicolas|      5|   67|    9|English|      A|   Nicolas_A|\n",
      "+-------+-------+-----+-----+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combination of column into new_col\n",
    "\n",
    "df.withColumn('Marks_mul_class', df.Marks*df.Class).show(5)\n",
    "\n",
    "df.withColumn('Name_section', concat(df.Name,df.Section)).show(5)\n",
    "\n",
    "#concat_ws() function of Pyspark concatenates multiple string columns into a single column with a given separator or delimiter.\n",
    "df.withColumn('Name_section', concat_ws('_',df.Name,df.Section)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee71813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----+-------+-------+\n",
      "|   Name|Roll_No|Grade|Class|Subject|Section|\n",
      "+-------+-------+-----+-----+-------+-------+\n",
      "|  Avind|      1|   92|    9|  Maths|      A|\n",
      "| Aditya|      2|   87|    9|  Maths|      A|\n",
      "|   John|      3|   23|    9|  Maths|      B|\n",
      "|   Mary|      4|   45|    9|  Maths|      B|\n",
      "|Nicolas|      5|   67|    9|English|      A|\n",
      "+-------+-------+-----+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename Column\n",
    "\n",
    "df.withColumnRenamed(\"Marks\",\"Grade\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1dc477d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-------+\n",
      "|   Name|Marks| Subject|Section|\n",
      "+-------+-----+--------+-------+\n",
      "|  Avind|   92|   Maths|      A|\n",
      "| Aditya|   87|   Maths|      A|\n",
      "|   John|   23|   Maths|      B|\n",
      "|   Mary|   45|   Maths|      B|\n",
      "|Nicolas|   67| English|      A|\n",
      "|  Jonny|  100| English|      A|\n",
      "|    Tom|   55| English|      B|\n",
      "|   Yash|   32| English|      B|\n",
      "|Pushkar|   30| Science|      B|\n",
      "|  Parth|   76| Science|      A|\n",
      "| Piyush|   86| Science|      A|\n",
      "| Zodiac|   65|Computer|      B|\n",
      "+-------+-----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop column\n",
    "\n",
    "df.drop('Roll_No','Class').show()\n",
    "\n",
    "#Note: there is not concept of axis here , so for row wise drop we use filter and where in next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1033c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "997dd53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Dept: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+--------+----+------+\n",
      "|    Name|Dept|Salary|\n",
      "+--------+----+------+\n",
      "|     Ram| Dev| 30000|\n",
      "|   Shyam| Dev| 31000|\n",
      "|    Babu| Dev| 32000|\n",
      "|     Ram| Sup| 40000|\n",
      "|   Shyam| Sup| 31000|\n",
      "|    Babu| Sup| 32000|\n",
      "|Anuradha| Dev| 30000|\n",
      "|    Mina| Dev| 33000|\n",
      "|Anuradha| Sup| 40000|\n",
      "|    Mina| Sup| 33000|\n",
      "|     Ram| Dev| 30000|\n",
      "+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Ram','Dev',30000),\n",
    "        ('Shyam','Dev',31000),\n",
    "        ('Babu','Dev',32000),\n",
    "        ('Ram','Sup',40000),\n",
    "        ('Shyam','Sup',31000),\n",
    "        ('Babu','Sup',32000),\n",
    "        ('Anuradha','Dev',30000),\n",
    "        ('Mina','Dev',33000),\n",
    "        ('Anuradha','Sup',40000),\n",
    "        ('Mina','Sup',33000),\n",
    "        ('Ram','Dev',30000)\n",
    "       ]\n",
    "column = ['Name','Dept','Salary']\n",
    "new_df = spark.createDataFrame(data = data, schema= column)\n",
    "new_df.printSchema()\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3917ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Record Count:  11\n",
      "Distinct count: 10\n",
      "Distinct count: 10\n",
      "+--------+----+------+\n",
      "|    Name|Dept|Salary|\n",
      "+--------+----+------+\n",
      "|   Shyam| Dev| 31000|\n",
      "|     Ram| Dev| 30000|\n",
      "|    Babu| Dev| 32000|\n",
      "|   Shyam| Sup| 31000|\n",
      "|     Ram| Sup| 40000|\n",
      "|    Babu| Sup| 32000|\n",
      "|Anuradha| Dev| 30000|\n",
      "|    Mina| Dev| 33000|\n",
      "|    Mina| Sup| 33000|\n",
      "|Anuradha| Sup| 40000|\n",
      "+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Record Count: \", new_df.count())  # Last row of record is similiar to the first one (1 duplicate row)\n",
    "\n",
    "# way 1\n",
    "df1 = new_df.distinct()\n",
    "print(\"Distinct count using distinct: \"+str(df1.count()))\n",
    "\n",
    "#way 2\n",
    "df2 = new_df.dropDuplicates()\n",
    "print(\"Distinct count using dropDuplicates: \"+str(df2.count()))\n",
    "\n",
    "df2.show() #show any df1/df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e05a6e",
   "metadata": {},
   "source": [
    "Note : PySpark doesn’t have a distinct method that takes columns that should run distinct on (drop duplicate rows on selected multiple columns) however, it provides another signature of dropDuplicates() function which takes multiple columns to eliminate duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dbc19914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of Dept & Salary : 8\n",
      "+-----+----+------+\n",
      "| Name|Dept|Salary|\n",
      "+-----+----+------+\n",
      "|  Ram| Dev| 30000|\n",
      "|Shyam| Dev| 31000|\n",
      "| Babu| Dev| 32000|\n",
      "| Mina| Dev| 33000|\n",
      "|Shyam| Sup| 31000|\n",
      "| Babu| Sup| 32000|\n",
      "| Mina| Sup| 33000|\n",
      "|  Ram| Sup| 40000|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find duplicated based on multi-columns\n",
    "\n",
    "df3 = new_df.dropDuplicates([\"Dept\",\"Salary\"])\n",
    "print(\"Distinct count of Dept & Salary : \"+str(df3.count()))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b9d5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note : To save these results we can assign it same df or new df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
