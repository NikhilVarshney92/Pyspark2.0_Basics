{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709e734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Basic imports\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#session Built\n",
    "\n",
    "spark = SparkSession.builder.appName('Basics_pyspark_df').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7d91ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby on one column\n",
    "\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1a3e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+\n",
      "|department|state|sum(salary)|\n",
      "+----------+-----+-----------+\n",
      "|Sales     |NY   |176000     |\n",
      "|Sales     |CA   |81000      |\n",
      "|Finance   |CA   |189000     |\n",
      "|Finance   |NY   |162000     |\n",
      "|Marketing |NY   |91000      |\n",
      "|Marketing |CA   |80000      |\n",
      "+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby on multi-columns\n",
    "\n",
    "df.groupBy([\"department\",\"state\"]).sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8c1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      90000|\n",
      "|   Finance|      99000|\n",
      "| Marketing|      91000|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+----------+\n",
      "|department|min(bonus)|\n",
      "+----------+----------+\n",
      "|     Sales|     10000|\n",
      "|   Finance|     15000|\n",
      "| Marketing|     18000|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "df.groupBy(\"department\").count().show()\n",
    "\n",
    "#max\n",
    "df.groupBy(\"department\").max(\"salary\").show()\n",
    "\n",
    "#min\n",
    "df.groupBy(\"department\").min(\"salary\").show()\n",
    "\n",
    "#average\n",
    "df.groupBy(\"department\").min(\"bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0887612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|     Sales|   NY|     176000|     30000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "|   Finance|   CA|     189000|     47000|\n",
      "|   Finance|   NY|     162000|     34000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#multi-columns and multi-agg\n",
    "\n",
    "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b834207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using filter/where with groupby    #Note: this agg + where/filter works same as HAVING clause in SQL.\n",
    "\n",
    "df.groupBy(\"department\").agg(\n",
    "    sum(\"salary\").alias(\"sum_salary\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "    max(\"bonus\").alias(\"max_bonus\")).where(col(\"sum_bonus\") >= 50000).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323f3d0",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899eebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---------+\n",
      "|Dept_No|Dept_Name|Dept_Code|Dept_Head|\n",
      "+-------+---------+---------+---------+\n",
      "|      1|    Sales|     SL01|    Mohit|\n",
      "|      2|  Finance|     FN05|     Ravi|\n",
      "|      3|Marketing|     MK04|  Shekhar|\n",
      "+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [(1,'Sales','SL01','Mohit'),\n",
    "             (2,'Finance','FN05','Ravi'),\n",
    "             (3,'Marketing','MK04','Shekhar')]\n",
    "dept_col = ['Dept_No','Dept_Name','Dept_Code','Dept_Head']\n",
    "\n",
    "dept_df = spark.createDataFrame(data=dept_data, schema = dept_col)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c24061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+------+---+-----+-------+---------+---------+---------+\n",
      "|employee_name|state|salary|age|bonus|Dept_No|Dept_Name|Dept_Code|Dept_Head|\n",
      "+-------------+-----+------+---+-----+-------+---------+---------+---------+\n",
      "|        Maria|   CA| 90000| 24|23000|      2|  Finance|     FN05|     Ravi|\n",
      "|        Raman|   CA| 99000| 40|24000|      2|  Finance|     FN05|     Ravi|\n",
      "|        Scott|   NY| 83000| 36|19000|      2|  Finance|     FN05|     Ravi|\n",
      "|          Jen|   NY| 79000| 53|15000|      2|  Finance|     FN05|     Ravi|\n",
      "|         Jeff|   CA| 80000| 25|18000|      3|Marketing|     MK04|  Shekhar|\n",
      "|        Kumar|   NY| 91000| 50|21000|      3|Marketing|     MK04|  Shekhar|\n",
      "|        James|   NY| 90000| 34|10000|      1|    Sales|     SL01|    Mohit|\n",
      "|      Michael|   NY| 86000| 56|20000|      1|    Sales|     SL01|    Mohit|\n",
      "|       Robert|   CA| 81000| 30|23000|      1|    Sales|     SL01|    Mohit|\n",
      "+-------------+-----+------+---+-----+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(dept_df, df.department == dept_df.Dept_Name, 'inner').drop('department').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19814d",
   "metadata": {},
   "source": [
    "### union\n",
    "\n",
    "Dataframe union() – union() method of the DataFrame is used to merge two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\n",
    "\n",
    "\n",
    "DataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5299b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---------+\n",
      "|Dept_No|Dept_Name|Dept_Code|Dept_Head|\n",
      "+-------+---------+---------+---------+\n",
      "|      1|    Sales|     SL01|    Mohit|\n",
      "|      2|  Finance|     FN05|     Ravi|\n",
      "|      3|Marketing|     MK04|  Shekhar|\n",
      "+-------+---------+---------+---------+\n",
      "\n",
      "+-------+-----------+---------+---------+\n",
      "|Dept_No|  Dept_Name|Dept_Code|Dept_Head|\n",
      "+-------+-----------+---------+---------+\n",
      "|      1|      Sales|     SL01|    Mohit|\n",
      "|      4|         IT|     IT05|    Vinod|\n",
      "|      5|Consultancy|     CL04|   Suresh|\n",
      "+-------+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [(1,'Sales','SL01','Mohit'),\n",
    "             (2,'Finance','FN05','Ravi'),\n",
    "             (3,'Marketing','MK04','Shekhar')]\n",
    "dept_col = ['Dept_No','Dept_Name','Dept_Code','Dept_Head']\n",
    "\n",
    "dept_df = spark.createDataFrame(data=dept_data, schema = dept_col)\n",
    "dept_df.show()\n",
    "\n",
    "dept_data2 = [(1,'Sales','SL01','Mohit'),\n",
    "             (4,'IT','IT05','Vinod'),\n",
    "             (5,'Consultancy','CL04','Suresh')]\n",
    "dept_col2 = ['Dept_No','Dept_Name','Dept_Code','Dept_Head']\n",
    "\n",
    "dept_df2 = spark.createDataFrame(data=dept_data2, schema = dept_col2)\n",
    "dept_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a738be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+---------+\n",
      "|Dept_No|  Dept_Name|Dept_Code|Dept_Head|\n",
      "+-------+-----------+---------+---------+\n",
      "|      1|      Sales|     SL01|    Mohit|\n",
      "|      2|    Finance|     FN05|     Ravi|\n",
      "|      3|  Marketing|     MK04|  Shekhar|\n",
      "|      1|      Sales|     SL01|    Mohit|\n",
      "|      4|         IT|     IT05|    Vinod|\n",
      "|      5|Consultancy|     CL04|   Suresh|\n",
      "+-------+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.union(dept_df2).show()                     # if you notice union may have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd94df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+---------+\n",
      "|Dept_No|  Dept_Name|Dept_Code|Dept_Head|\n",
      "+-------+-----------+---------+---------+\n",
      "|      1|      Sales|     SL01|    Mohit|\n",
      "|      2|    Finance|     FN05|     Ravi|\n",
      "|      3|  Marketing|     MK04|  Shekhar|\n",
      "|      4|         IT|     IT05|    Vinod|\n",
      "|      5|Consultancy|     CL04|   Suresh|\n",
      "+-------+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to drop duplicates\n",
    "\n",
    "dept_df.union(dept_df2).distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a63cfea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|Ravi|\n",
      "|Ram|   2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[1,'Ravi']], [\"id\",\"name\"])\n",
    "df2 = spark.createDataFrame([['Ram',2]], [\"name\",\"id\"])\n",
    "\n",
    "# Note let's check the flaw in union\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da0816",
   "metadata": {},
   "source": [
    "The difference between unionByName() function and union() is that this function\n",
    "resolves columns by name (not by position). In other words, unionByName() is used to merge two DataFrames by column names instead of by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab630d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|Ravi|\n",
      "|  2| Ram|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionByName\n",
    "\n",
    "# Using allowMissingColumns\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a093a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|   5|   2|   6|null|\n",
      "|null|   6|   7|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with different column names\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Using allowMissingColumns\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bb0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
